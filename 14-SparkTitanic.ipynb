{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "D'abord, installation de Spark\n"
      ],
      "metadata": {
        "id": "Qsscs6pSvVr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "memory = '8g'\n",
        "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
      ],
      "metadata": {
        "id": "9VEPojkLvU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://angelo_crime_data/spark-3.2.1-bin-hadoop2.7.tgz ."
      ],
      "metadata": {
        "id": "HgC6Q2VbvdqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!rm spark-3.2.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "aaoOl_bDvhY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark pyspark"
      ],
      "metadata": {
        "id": "73-xwcuYvkka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"./spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Rbs70XHnvntP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk4d0on0uM1b"
      },
      "source": [
        "# First ML model using Apache Spark MLlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoMS1EffuM1d"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "To load the data we are using Spark DataFrames. Spark it’s a little bit more complicated than Pandas. You can’t just do “import -> read_csv()”. You first need to start a Spark Session, to do that write:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vo_hb0pBuVst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxYPySqFuM1d"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Titanic Data') \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMSOdcjHuM1e"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqRclet3uM1f"
      },
      "source": [
        "Cool! So now we have everything in place to read the data. To do so write:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lsteffenel/RT0902-IntroML/main/data/titanic_train.csv"
      ],
      "metadata": {
        "id": "wAT9YZ-SwIs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RUIpjPYuM1f"
      },
      "outputs": [],
      "source": [
        "df = (spark.read\n",
        "          .format(\"csv\")\n",
        "          .option('header', 'true')\n",
        "          .load(\"titanic_train.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c5W8APXuM1f"
      },
      "source": [
        "And that’s it! You have created your TITANIC Spark DataFrame. To see the internals of the DataFrame write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxNDMSO9uM1f"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EGyxkxzuM1g"
      },
      "source": [
        "One good thing about using Python is that you can interact with Pandas easily. And to show our data in a prettier format you can write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZE42bHiuM1g"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8vQojvTuM1g"
      },
      "source": [
        "## Checking information about your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02UQ6ckuM1h"
      },
      "outputs": [],
      "source": [
        "# How many rows we have\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwPmHcIBuM1h"
      },
      "outputs": [],
      "source": [
        "# The names of our columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-KmB5bOuM1h"
      },
      "outputs": [],
      "source": [
        "# Basics stats from our columns\n",
        "df.describe().toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o6XnL_muM1h"
      },
      "outputs": [],
      "source": [
        "# Types of our columns\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofzyV4yRuM1h"
      },
      "source": [
        "## Data preparation and feature engineering\n",
        "\n",
        "One of the things we noticed from the data exploration from above was that all the columns were of String type. But that doesn’t seem right. Some of them should be numeric. So we are going to cast them. Also because of time I’m only selecting a few variables for modeling so we don’t have to deal with the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeI1PGn0uM1i"
      },
      "outputs": [],
      "source": [
        "# Cast numeric columns\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "dataset = df.select(col('Survived').cast('float'),\n",
        "                         col('Pclass').cast('float'),\n",
        "                         col('Sex'),\n",
        "                         col('Age').cast('float'),\n",
        "                         col('Fare').cast('float'),\n",
        "                         col('Embarked')\n",
        "                        )\n",
        "\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9bZoX3nuM1i"
      },
      "outputs": [],
      "source": [
        "## See if we have missing values\n",
        "from pyspark.sql.functions import isnull, when, count, col\n",
        "\n",
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb670uFOuM1i"
      },
      "source": [
        "We see that we also have null values in some columns, so we will just eliminate them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4InGMMTWuM1i"
      },
      "outputs": [],
      "source": [
        "# Drop missing values\n",
        "dataset = dataset.replace('null', None)\\\n",
        "        .dropna(how='any')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhLz9yEFuM1j"
      },
      "source": [
        "Now, the Spark ML library only works with numeric data. But we still want to use the Sex and the Embarked column. For that, we will need to encode them. To do it let’s use something called the StringIndexer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwE4fQO8uM1j"
      },
      "outputs": [],
      "source": [
        "# Index categorical columns with StringIndexer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Sex', \n",
        "    outputCol='Gender', \n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Embarked', \n",
        "    outputCol='Boarded', \n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8kvpsgvuM1j"
      },
      "source": [
        "As you can see we’ve created two new columns “Gender” and “Boarded” that contain the same information as “Sex” and “Embarked” but now they are numeric. Let’s do a final check for our data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4fsDauDuM1j"
      },
      "outputs": [],
      "source": [
        "# Check data types\n",
        "dataset.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QataSm2quM1j"
      },
      "source": [
        "So all the columns we want are numeric. We now have to get rid of the old columns “Sex” and “Embarked” because we won’t be using them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8OGnzFduM1j"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "dataset = dataset.drop('Sex')\n",
        "dataset = dataset.drop('Embarked')\n",
        "\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBVY33x3uM1j"
      },
      "source": [
        "Jut one step left before going into the machine learning part. Spark actually works to predict with a column with all the features smashed together into a list-like structure. \n",
        "\n",
        "But you want to predict “Survived”, so you need to combine the information of the columns “Pclass”, “Age”, “Fare”, “Gender” and “Boarded” into one column. To do that in Spark we use the VectorAssembler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYlFeHz8uM1k"
      },
      "outputs": [],
      "source": [
        "# Assemble all the features with VectorAssembler\n",
        "\n",
        "required_features = ['Pclass',\n",
        "                    'Age',\n",
        "                    'Fare',\n",
        "                    'Gender',\n",
        "                    'Boarded'\n",
        "                   ]\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
        "\n",
        "transformed_data = assembler.transform(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIjplgCDuM1k"
      },
      "outputs": [],
      "source": [
        "transformed_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDP06qkMuM1k"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Now for the fun part right? NO! Haha. Modeling is important but without all the previous steps it would be impossible. So have fun in all the steps :)\n",
        "Before modeling let’s do the usual splitting between training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0klt7BLuM1k"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Eu1dfNauM1k"
      },
      "source": [
        "Ok. Modeling. That means, in this case, build and fit an ML model to our dataset to predict the “Survived” columns with all the other ones. We will be using a Random Forest Classifier. This is actually an estimator that we have to fit.\n",
        "This is actually the easy part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQghs0GpuM1k"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol='Survived', \n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCNNrV7VuM1k"
      },
      "source": [
        "Now we fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qebYtFDIuM1k"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model = rf.fit(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THxWO7CcuM1k"
      },
      "source": [
        "This will give us something called a transformer. And finally, we predict using the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7H8VY30uM1k"
      },
      "outputs": [],
      "source": [
        "# Predict with the test dataset\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B6QLYpauM1l"
      },
      "source": [
        "And that’s it! You did it. Congratulations :). Your first Spark ML model. \n",
        "\n",
        "Now let’s see how well we did. For that, we will use a basic metric called the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXHEey1RuM1l"
      },
      "outputs": [],
      "source": [
        "# Evaluate our model\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='Survived', \n",
        "    predictionCol='prediction', \n",
        "    metricName='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kL48OgFuM1l"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Test Accuracy = ', accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}