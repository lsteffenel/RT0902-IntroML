{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier 1 : Apprentissage et classification Automatique \n",
    "\n",
    "## A : Apprenstissage non-supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeu de données :\n",
    "Le jeu de données Iris a été utilisé à l''article classique de Fisher, publié en 1936, intitulé \"L'utilisation de plusieurs mesures dans des problèmes taxonomiques\", est également disponible dans le référentiel UCI Machine Learning.\n",
    "\n",
    "Il comprend trois espèces d’iris de 50 échantillons chacune, ainsi que des propriétés propres à chaque fleur. Une espèce de fleur est séparable linéairement des deux autres, mais les deux autres ne sont pas séparables linéairement l'une de l'autre.\n",
    "\n",
    "Les colonnes de cet ensemble de données sont:\n",
    "\n",
    "     Id\n",
    "     Longueur du Sépale Cm\n",
    "     Largeur du Sépale Cm\n",
    "     Longueur du Pétale cm\n",
    "     Largeur du Pétale Cm\n",
    "     Espèce : classe : Iris Setosa, Iris Versicolor ou Iris Virginica.\n",
    "\n",
    "Un échantillon : (4.9,3.6,1.4,0.1, “Iris-setosa”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regroupement (Clustering) et visualisation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des données\n",
    "Avec la fonction read_csv de Pandas: on peut mettre dans notre dataframe le contenu du fichier csv, en indiquant comme paramètre (1: le chemin ou la source où se trouve le fichier csv, 2: les séparateurs entre les valeurs dans notre cas ces des vergules) en troisième position, un paramètre facultatif pour spécifier le type d'encodage de notre fichier exemple encoding =\"UTF8\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('datasets/Iris.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df[['SepalLengthCm', 'SepalWidthCm', \n",
    "                  'PetalLengthCm', 'PetalWidthCm']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des types de fleurs, selon un découpage de variables par paires, en utilisant l'outil pairplot de seaborn. \n",
    "\n",
    "Vous pouvez consulter la documentation sur : https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main flowers species visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.pairplot(df.drop(\"Id\", axis=1), hue=\"Species\")  #,  diag_kind=False\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs de distribution des principales dimensions de plantes \n",
    "\n",
    "df.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering par l'algorithme K-means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la classe (Species) est une chaine de caractères. Pour pouvoir représenter cette information en un tableau ou shéma, il faut transformer ces valeurs en des valeurs entiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour voir qulles sont les types de fleurs\n",
    "df['Species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut réaliser cette opération en utilisant le __Label Encoder__ comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targetss = df['Species'].ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Species'] = le.fit_transform(df['Species'])\n",
    "df_labels = df['Species']\n",
    "\n",
    "df_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3, random_state=10)\n",
    "km.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation des clusters\n",
    "plt.scatter(df_features.PetalLengthCm, df_features.PetalWidthCm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap=np.array(['Red','green','blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation des clusters réels\n",
    "plt.scatter(df_features.PetalLengthCm, df_features.PetalWidthCm,\n",
    "            c=colormap[df_labels],s=40)\n",
    "plt.title('Clustering réel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation des clusters prédits\n",
    "plt.scatter(df_features.PetalLengthCm, df_features.PetalWidthCm,\n",
    "            c=colormap[km.labels_],s=40)\n",
    "plt.title('Clustering prédit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut visualiser le clustering avec les centroids : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_\n",
    "plt.scatter(centroids[:, 2], centroids[:, 3],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='orange', zorder=10)\n",
    "plt.scatter(df_features.PetalLengthCm, df_features.PetalWidthCm,\n",
    "            c=colormap[km.labels_],s=40)\n",
    "\n",
    "plt.title('Clustering prédit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation du Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Kmeans algorithm and get the index of data points clusters\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(df_features)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    #for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans (n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict(df_features)\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    score = silhouette_score (df_features, preds, metric='euclidean')\n",
    "    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation 3D des clusters\n",
    "Si on veut visaliser en 3D les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X = df.drop(\"Id\", axis=1).drop(\"Species\", axis=1).values\n",
    "y = df_labels\n",
    "\n",
    "#centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "centers = [[0, 0], [0, 0], [0, 0]]\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(1, figsize=(5, 4))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "for name, label in [('Iris-setosa', 0),\n",
    "                    ('Iris-versicolour', 1),\n",
    "                    ('Iris-virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
    "\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. La classification supervisée : \n",
    "C’est l’opération qui permet de placer chaque individu de la population dans une classe parmi l’ensemble des classes préétablies, en suivant un processus d’apprentissage supervisé. \n",
    "le choix de la classe d’un individu dépend de ses caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithme KNN (K Nearest Neighbors)\n",
    "- Arbre de décision \n",
    "- Machine à vecteurs de support (SVM)\n",
    "- Réseau de neurones\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problématique:\n",
    "#### Données :\n",
    "- Une liste d’exemples X {1..n} caractérisés par un ensemble d’attributs P. \n",
    "- Un ensemble C de classes préétablies.   \n",
    "- Les caractéristiques d'un nouvel exemple «newX».\n",
    "#### Question :\n",
    "-  Quelle est la classe appropriée à «newX» ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeu de données :\n",
    "Le jeu de données Iris a été utilisé à l''article classique de Fisher, publié en 1936, intitulé \"L'utilisation de plusieurs mesures dans des problèmes taxonomiques\", est également disponible dans le référentiel UCI Machine Learning.\n",
    "\n",
    "Il comprend trois espèces d’iris de 50 échantillons chacune, ainsi que des propriétés propres à chaque fleur. Une espèce de fleur est séparable linéairement des deux autres, mais les deux autres ne sont pas séparables linéairement l'une de l'autre.\n",
    "\n",
    "Les colonnes de cet ensemble de données sont:\n",
    "\n",
    "     Id\n",
    "     Longueur du Sépale Cm\n",
    "     Largeur du Sépale Cm\n",
    "     Longueur du Pétale cm\n",
    "     Largeur du Pétale Cm\n",
    "     Espèce : classe : Iris Setosa, Iris Versicolor ou Iris Virginica.\n",
    "\n",
    "Un échantillon : (4.9,3.6,1.4,0.1, “Iris-setosa”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. importation des librairies\n",
    "Avec Pandas on peut manipuler lire (et/ou écrire) nos jeux de données, généralement avec une extension .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des lib \n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Importation des données\n",
    "Avec la fonction read_csv de Pandas: on peut mettre dans notre dataframe le contenu du fichier csv, en indiquant comme paramètre (1: le chemin ou la source où se trouve le fichier csv, 2: les séparateurs entre les valeurs dans notre cas ces des vergules) en troisième position, un paramètre facultatif pour spécifier le type d'encodage de notre fichier exemple encoding =\"UTF8\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/Iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "Quelle est la moyenne de la longueure des petales de la setosa ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reponse 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il y a plein de manière d'écrire cette commande\n",
    "rep = df[df[\"Species\"]=='Iris-setosa'].PetalLengthCm.mean()\n",
    "\n",
    "#df[df.Outcome==1].SkinThickness.mean()\n",
    "#df[df[\"Outcome\"]==1][\"SkinThickness\"].mean()\n",
    "\n",
    "rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2\n",
    "Quelle est la longueure maximale des sepales de la setosa ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = df[df[\"Species\"]=='Iris-setosa'].SepalLengthCm.max()\n",
    "\n",
    "rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Statistiques descriptives élémentaires\n",
    "Lire les informations sur nos données (Types d'attributs, valeurs manquantes...) Pandas nous permet de voir les informations sur notre benchmark exemple: avec dataframe.info() il nous affiche tout les attributs de notre fichier avec le type de donnée et le nombre de valeurs de chaque colonne\n",
    "dataframe.columns permet de citer les noms de toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #donner les infos de notre data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On peut supprimer la colonne ID :\n",
    "\n",
    "df.drop('Id',axis=1,inplace=True) \n",
    "\n",
    "#dropping the Id column as it is unecessary, axis=1 specifies that it should be column wise, inplace =1 means the changes should be reflected into the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. préparation des données\n",
    "Dans cette étape nous déterminons les attributs choisis pour l'entrainement et nous définissons l'attribut \"classe\" de notre benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir les attraibuts qui nous intéréssent \n",
    "df_features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir l'attribut classe\n",
    "df_labels = df[['Species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut schématiser la distribution des classes, il suffit de faire appel à la libraire seaborn, en suite définir l'attribut concerné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "# schématiser la distribution des classes\n",
    "sns.countplot(df['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Transformer la colonne des classes en labels numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelss=df['Species']#.ravel()\n",
    "df_labelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Species'] = le.fit_transform(df['Species'])\n",
    "df_labels = df['Species']\n",
    "\n",
    "df_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Diviser le dataset en données d'entrainement et données de teste\n",
    "Ceci est réalisable avec sklearn qui permet de prendre aléatoirement des données de teste à partir du benchmark et laisser le reste pour l'apprentissage.\n",
    "La fonction train_test_split(param1,param2,param3,param4) prends 4 paramétres:\n",
    "le premier dédié à l'ensemble d'entrainement, le deuxième à l'ensemble de teste, le troisième c'est le paramètre du % de l'ensemble de test (généralement entre 15 et 40%), \n",
    "\n",
    "le 4 ème paramétre (facultatif) pour spécifier quel type de fonction random utiliser:\n",
    "si vous utilisez random_state = some_number, vous pouvez garantir que la sortie de Run 1 sera égale à la sortie de Run 2, c'est-à-dire que votre split sera toujours le même. Peu importe ce que le nombre réel random_state est 42, 0, 21, ... L'important est que chaque fois que vous utilisez 42, vous obtiendrez toujours la même sortie la première fois que vous faites la division. Ceci est utile si vous voulez des résultats reproductibles, par exemple dans la documentation, afin que tout le monde puisse toujours voir les mêmes nombres lors de l'exécution des exemples.\n",
    "\n",
    "Cette fonction retourne 4 sorties: \n",
    "La 1ere est le sous-ensembles aléatoires d'entrainement \n",
    "La 2éme est le vecteur de leurs labels (leurs classes).\n",
    "La 3ème est le sous-ensemble aléatoire pour le teste.\n",
    "La 4ème est le vecteur de leurs labels (leurs classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#decouper le data set en 30% pour test et 70% pour train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, \n",
    "                                                    df_labels, test_size=0.4,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".shape permet de savoir la dimension d'un ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', X_train.shape) # .shape permet de voir la\n",
    "print('x_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode des K plus proches Voisins ( K nearest neigbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # le classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definir l'algorithme que je veux utiliser (KNN) avec le paramètre k=3\n",
    "mon_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#fitting : Lancer l'apprentissage ( données,labels)\n",
    "mon_knn.fit(X_train, y_train)#.values.ravel())\n",
    "# Evaluer l'entrainement de mon modèle\n",
    "train_score = mon_knn.score(X_train, y_train)\n",
    "print('train score = ',train_score )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('---- L Ensemble de test ----- \\n',X_test)\n",
    "#ypred : contient les prédictions de l'ensemble de teste\n",
    "ypred = mon_knn.predict(X_test)\n",
    "print('---- Les classes prédites par mon Algo ----- \\n',ypred)\n",
    "print('---- Les classes réelles ----- \\n',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle\n",
    "#### A. Accuracy : \n",
    "Documentation sur accuracy_score de sk-learn ici : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy%20score#sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # Evaluation\n",
    "print ('KNN accuracy score')\n",
    "print (accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Par Validation Croisée :\n",
    "Documentation sur la validation croisée de sk-learn ici : https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(mon_knn, X_train, y_train, cv=5)\n",
    "#scores = cross_val_score(mon_knn, df_features, df_labels, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"par validation croisée:  \" , scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Recall, Precision et F-score:\n",
    "documentation sur recall/ precision \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall%20score#sklearn.metrics.recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score # Evaluation\n",
    "print ('KNN recall score')\n",
    "print (recall_score(y_test, ypred, average=None))\n",
    "print ('KNN precision score')\n",
    "print (precision_score(y_test, ypred,average=None))\n",
    "print ('f1 score')\n",
    "print (f1_score(y_test, ypred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation sur la F-score : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1%20score#sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Par matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix\n",
    "print(confusion_matrix(y_test, ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False,  title=' confusion matrix ', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = mon_knn.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "#Y_pred_classes = np.argmax(Y_pred , axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = y_test#np.argmax(y_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    " \n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode des arbres de décision\n",
    "Je vous invite à consulter la documentation détaillée de cette méthode sur le site de sklearn : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=treeclassifier#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De la meme manière que pour le KNN\n",
    "\n",
    "#importer l'algorithme tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "#fitting : Lancer l'apprentissage ( données,labels)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluer l'entrainement de mon modèle\n",
    "train_score = clf.score(X_train, y_train)\n",
    "print('train score = ',train_score )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ypred : contient les prédictions de l'ensemble de teste\n",
    "ypred = clf.predict(X_test)\n",
    "\n",
    "print ('Decision tree accuracy score')\n",
    "print (accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ploter mon arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tree.DecisionTreeClassifier(max_depth=3)\n",
    "tree.plot_tree(clf.fit(df_features, df_labels),max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Une autre manière de schématiser un arbre de decision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree.export import export_text\n",
    "from sklearn import tree\n",
    "algo_tree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "algo_tree = algo_tree.fit(df_features, df_labels)\n",
    "r = export_text(clf , feature_names = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm' ])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(X_test)\n",
    "\n",
    "print ('Tree accuracy score')\n",
    "\n",
    "print (accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = confusion_matrix(Y_true, ypred)\n",
    "\n",
    " \n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice  :\n",
    "En se basant sur ce notebook :\n",
    "- Ajouter un code qui cherche les meilleurs paramètres pour chaque méthode. ( vous pouvez utiliser gridsearch)\n",
    "- Ajouter d'autre méthodes de classification à ce notebook ( exmple: Naive Bayes, SVM, Random Forest, Réseaux de neurones multi-couches ... etc)\n",
    "- Evaluer toutes vos méthodes par validation croisée (nbr de paquet = 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
