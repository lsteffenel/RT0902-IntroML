{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/RT0902-IntroML/blob/main/15-Chicago_crime_data_on_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5NGFmN8B11c"
      },
      "source": [
        "# Chicago crime dataset analysis\n",
        "---\n",
        "\n",
        "Ce notebook permet d'appliquer un peu de vos connaissances √† la d√©couverte d'un vrai dataset.\n",
        "\n",
        "Vous allez effectuer :\n",
        " * Lecture, transformation et requ√™tage avec Apache Spark.\n",
        " * Parfois, transformer les donn√©es en Pandas pour une meilleure visualisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orsg3O-SB11e"
      },
      "source": [
        "---\n",
        "\n",
        "## Quelques Import\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zbNyaRuB11f"
      },
      "source": [
        "Import de Pandas et Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQft1Nw1B11g"
      },
      "outputs": [],
      "source": [
        "## standard imports\n",
        "import pandas as pnd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWzoN7XpB11e"
      },
      "source": [
        "Spark imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG1K0VaslXFL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "memory = '8g'\n",
        "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewSQBpTqB11f"
      },
      "outputs": [],
      "source": [
        "## spark imports\n",
        "from pyspark.sql import Row, SparkSession\n",
        "import pyspark.sql.functions as pyf\n",
        "\n",
        "#spark = SparkSession.builder.master(\"local[1]\").appName(\"RT0902\").getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"RT0902\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkJN_E5AB11h"
      },
      "source": [
        "---\n",
        "## Dataset\n",
        "Les donn√©es originales viennent de Kaggle (https://www.kaggle.com/djonafegnem/chicago-crime-data-analysis)\n",
        "\n",
        "On trouve une liste de crimes registr√©s par le d√©partement de police de Chicago.\n",
        "\n",
        "Le dataset \"r√©el\" contient 4 fichiers pour des crimes allant de 2001 √† 2017.\n",
        "Comme le traitement de ces fichiers est long et demandeur en ressources, vous allez d'abord travailler avec un fichier r√©duit, qui ne contient que des donn√©es de 2001.\n",
        "\n",
        "Une fois votre code \"valid√©\", vous pouvez l'utiliser sur le cloud pour traiter l'ensemble de fichiers de la police.\n",
        "\n",
        "Ci-dessous vous trovez une description des diff√©rents champs des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2F-rS7vB11h"
      },
      "outputs": [],
      "source": [
        "content_cols = '''\n",
        "ID - Unique identifier for the record.\n",
        "Case Number - The Chicago Police Department RD Number (Records Division Number), which is unique to the incident.\n",
        "Date - Date when the incident occurred. this is sometimes a best estimate.\n",
        "Block - The partially redacted address where the incident occurred, placing it on the same block as the actual address.\n",
        "IUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.\n",
        "Primary Type - The primary description of the IUCR code.\n",
        "Description - The secondary description of the IUCR code, a subcategory of the primary description.\n",
        "Location Description - Description of the location where the incident occurred.\n",
        "Arrest - Indicates whether an arrest was made.\n",
        "Domestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\n",
        "Beat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area ‚Äì each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.\n",
        "District - Indicates the police district where the incident occurred. See the districts at https://data.cityofchicago.org/d/fthy-xz3r.\n",
        "Ward - The ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.\n",
        "Community Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https://data.cityofchicago.org/d/cauq-8yn6.\n",
        "FBI Code - Indicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\n",
        "X Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Y Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Year - Year the incident occurred.\n",
        "Updated On - Date and time the record was last updated.\n",
        "Latitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Longitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Location - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEV6TMBnB11k"
      },
      "source": [
        "### Donn√©es\n",
        "\n",
        "Les donn√©es seront t√©l√©charg√©es et stock√©es dans `./data/`. Ce sont des fichiers .CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dzWdo_JWJgw"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q9V0tdZWvN5"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r gs://angelo_crime_data/*.csv ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyafw6P0B11l"
      },
      "outputs": [],
      "source": [
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYYYo3wB11l"
      },
      "source": [
        "---\n",
        "## Lecture des donn√©es\n",
        "\n",
        "Avec l'op√©ration `csv read` de spark, nous allons lire et parser les fichiers. Le r√©sultat sera un seul DataFrame :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM8fs9TgB11l"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.csv('gs://angelo_crime_data/Chicago_*.csv', inferSchema=True, header=True)\n",
        "df = spark.read.csv('data/mini_data.csv', inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz6WzWjsl1H1"
      },
      "source": [
        "Note : ce qui prend vraiment le temps est la d√©couverte du sch√©ma : on n'a pas tellement de lignes, apr√®s tout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lSK8YawB11m"
      },
      "outputs": [],
      "source": [
        "# Affichage du sch√©ma (structure) du dataframe\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diff√©rences entre Pandas et Spark\n",
        "Pandas a des op√©rations telles que `info()` et `describe()`. Dans Spark, on n'a que `describe()`, qui n'est pas comparable √† celle de Pandas : il affiche plut√¥t le type des donn√©es, un peu comme `printSchema()`."
      ],
      "metadata": {
        "id": "ri4XZLFrDMu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "_PjKFVEaDkY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi9n1CYaB11n"
      },
      "source": [
        "Certaines lignes de n'ont aucune valeur d√©clar√©e √† la colonne `location_description` . C'est le moment de les supprimer.\n",
        "\n",
        "Pour cela, nous allons filtrer les entr√©es vides ('') en utilisant la fonction **`Dataset.filter`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LQyYPtcbtnQ"
      },
      "outputs": [],
      "source": [
        "df = df.filter(df['location_description'] != '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_YRLdkB11n"
      },
      "source": [
        "Un petit aper√ßu du d√©but du dataframe :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym9AaXn0B11n"
      },
      "outputs": [],
      "source": [
        "df.show(n=3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a quand m√™me plus de 560 mille entr√©es sur le petit fichier !! üòµ"
      ],
      "metadata": {
        "id": "XhAVXvqjEuvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Ek_ORlB11o"
      },
      "outputs": [],
      "source": [
        "print(df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykY8IxXaB11o"
      },
      "source": [
        "---\n",
        "## Comprendre les donn√©es"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types de Crime\n",
        "\n",
        "On veut conna√Ætre combien de types de crime (cat√©gories) existent dans le fichier."
      ],
      "metadata": {
        "id": "Mue99rjLC2-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_8EqKGnB11o"
      },
      "outputs": [],
      "source": [
        "# crime types\n",
        "crime_type_groups = df.groupBy('primary_type').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB7L2nGJB11o"
      },
      "outputs": [],
      "source": [
        "crime_type_counts = crime_type_groups.orderBy('count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrBK30fB11o"
      },
      "source": [
        "Jusqu'√† ici √ßa a √©t√© rapide : Spark fait une ex√©cution *lazy*, i.e., il n'a fait qu'enregistrer les *transformations* √† applier. Il attendra pour lancer l'ex√©cution uniquement lorsqu'une *action* est demand√©e (par exemple, afficher le r√©sultat).\n",
        "\n",
        "Dans la ligne suivante on demande le nombre total de lignes, mais en fait il va appliquer les modifications, faire le filtrage, etc. Sur un grand dataset, √ßa peut prendre pas mal de temps (d'o√π l'int√©r√™t de distribuer le travail entre plusieurs machines)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5allZo4IB11p"
      },
      "source": [
        "\n",
        "\n",
        "La commande suivante affiche les 20 types de crime les plus fr√©quents :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0G89nlsB11q"
      },
      "outputs": [],
      "source": [
        "crime_type_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nX9DLuuB11q"
      },
      "source": [
        "On peut faire un affichage plus propre (et d'autres op√©rations) en transformant ce dataframe en Pandas :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om_CpQ58oV0v"
      },
      "outputs": [],
      "source": [
        "counts_pddf = crime_type_counts.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMYfXn6obj0"
      },
      "outputs": [],
      "source": [
        "counts_pddf.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce dataset Pandas peut √™tre utilis√© pour une petite visualisation :"
      ],
      "metadata": {
        "id": "wX93myvEF4X1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeJrB5AIB11r",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
        "\n",
        "counts_pddf.sort_values('count').plot(kind='barh', x='primary_type', y='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya0LZ2NpB11r"
      },
      "source": [
        "### Convertir les dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbEroH7VB11s"
      },
      "source": [
        "**Si vous avez √©t√© attentif**, vous avez remarqu√© que les colonnes avec des dates ont √©t√© lues comme du texte simple :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "kdlf1-KDGh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2QC39keB11s"
      },
      "source": [
        "---\n",
        "En effet, le sch√©ma montrait que le champ `date` √©tait de type `string`, ce qui n'est pas tr√®s utile.\n",
        "\n",
        "Nous allons convertir ces dates au format timestamp.\n",
        "\n",
        "Nous allons changer ce format afin de le lire sous la forme '02/23/2006 09:06:22 PM' , c'est √† dire **`'MM/dd/yyyy hh:mm:ss a'`** (format am√©ricain).\n",
        "\n",
        "On va aussi rajouter une colonne `month` qui indique le premier jour du mois, sans l'heure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYzZl6iiB11s"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_timestamp, hour, trunc\n",
        "# d'abord, on convertit 'date' avec to_timestamp() et on enregistre cette valeur dans 'date_time'\n",
        "df = df.withColumn('date_time', to_timestamp('date', 'MM/dd/yyyy hh:mm:ss a'))\n",
        "# ensuite, on cr√©e une colonne 'month' √† partir de 'datetime')\n",
        "df = df.withColumn('month', trunc('date_time', 'month')) #adding a month column to be able to view stats on a monthly basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u35zw7vgB11s"
      },
      "outputs": [],
      "source": [
        "df.select(['date','date_time', 'month'])\\\n",
        "  .show(n=20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoeD7ctRB11s"
      },
      "source": [
        "### Combien d'arrestations ?\n",
        "\n",
        "Pas tous les crimes donnent lieu √† des arrestations.\n",
        "Gr√¢ce √† `groupBy`, nous allons afficher le nombre d'arrestations par mois (et le nombre de crimes sans arrestations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a-2HrFHB11s"
      },
      "outputs": [],
      "source": [
        "# On peut utiliser la colonne month pour affiche les quantit√©s d'arrestations au fil des ann√©es, group√©es par mois :\n",
        "type_arrest_date = df.groupBy(['arrest', 'month'])\\\n",
        "                     .count()\\\n",
        "                     .orderBy(['month', 'count'], ascending=[True, False])\n",
        "print()\n",
        "type_arrest_date.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox4oZ41EB11u"
      },
      "source": [
        "### Comment le nombre d'arrestations a √©volu√© sur l'ann√©e ?\n",
        "\n",
        "Pour l'afichage, nous allons encore une fois transformer notre dataset en Pandas. On transforme `type_arrest_date`, puis on utilise matplotlib pour l'affichage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: √† partir de type_arrest_date, g√©n√©rer un graphique matplotlib affichant le nombre arrests true et false, par mois\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for easier plotting\n",
        "type_arrest_date_pandas = type_arrest_date.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for arrest_status in type_arrest_date_pandas['arrest'].unique():\n",
        "    subset = type_arrest_date_pandas[type_arrest_date_pandas['arrest'] == arrest_status]\n",
        "    plt.plot(subset['month'], subset['count'], label=f'Arrest: {arrest_status}')\n",
        "\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Arrests')\n",
        "plt.title('Number of Arrests (True/False) per Month')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "08Ja7Z_YI6aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVUmuvVIB11u"
      },
      "source": [
        "### √Ä quel moment de la journ√©e les criminels sont plus actifs ?\n",
        "\n",
        "Ici c'est √† vous de refaire le m√™me type d'op√©ration. Je vais juste vous montrer comment cr√©er une colonne avec les heures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHDjY5bBB11v"
      },
      "outputs": [],
      "source": [
        "# Extract the \"hour\" field from the date into a separate column called \"hour\"\n",
        "df_hour = df.withColumn('hour', hour(df['date_time']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsZNEJXZB11v"
      },
      "outputs": [],
      "source": [
        "# Derive a data frame with crime counts per hour of the day:\n",
        "hourly_count = df_hour.groupBy(['primary_type', 'hour']).count()\n",
        "hourly_total_count = hourly_count.groupBy('hour').sum('count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVeHieTYB11v"
      },
      "outputs": [],
      "source": [
        "hourly_count_pddf = hourly_count.toPandas()\n",
        "hourly_total_count_pddf = hourly_total_count.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGf_KjlJB11v"
      },
      "outputs": [],
      "source": [
        "hourly_count_pddf = hourly_count_pddf.sort_values(by='hour')\n",
        "hourly_total_count_pddf = hourly_total_count_pddf.sort_values(by='hour')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_count_pddf.head(10)"
      ],
      "metadata": {
        "id": "mu728Fx4LU3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_total_count_pddf.head(10)"
      ],
      "metadata": {
        "id": "A4lzRYWNKKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S08QHOlBB11v"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(hourly_total_count_pddf['hour'], hourly_total_count_pddf['sum(count)'], label='Hourly Count')\n",
        "\n",
        "ax.set(xlabel='Hour of Day', ylabel='Total records',\n",
        "       title='Overall hourly crime numbers')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMZ2MN6dB11v"
      },
      "source": [
        "Il semble que c'est plus agit√© entre 18h et 22h... avec un pic √† midi.\n",
        "\n",
        "Regardons le d√©tail de chaque type de crime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group data by hour and primary type and sum the counts\n",
        "hourly_counts_grouped = hourly_count_pddf.groupby(['hour', 'primary_type'])['count'].sum().unstack()\n",
        "\n",
        "# Plot stacked area chart\n",
        "hourly_counts_grouped.plot(kind='area', stacked=True, ax=plt.gca())\n",
        "\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.title('Hourly Crime Counts by Primary Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Primary Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tGpzIUylLfCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6rHrROiB11w"
      },
      "source": [
        "### Dans que type d'endroit les crimes sont commis ?\n",
        "\n",
        "Le dataset indique la \"classe\" de lieu o√π le crime a √©t√© commis : maison, rue, etc. Regardons √ßa en d√©tails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBdTg6GRB11w"
      },
      "outputs": [],
      "source": [
        "# Combien de types d'endroit sont recens√©s\n",
        "df.select('location_description').distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsllA6BgB11w"
      },
      "source": [
        "Ok, il y a 114 types diff√©rents d'endroit qui sont recens√©s.\n",
        "\n",
        "Quels sont les 10 endroits les plus fr√©quents ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OpMDWEBB11w"
      },
      "outputs": [],
      "source": [
        "df.groupBy(['location_description']).count().orderBy('count', ascending=False).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey4LQDQWB11x"
      },
      "source": [
        "### Crimes \"domestiques\" :\n",
        "\n",
        "En dehors de la localit√©, le dataset indique aussi s'il s'agit d'une violence domestique (dispute familiale, harc√©lement, etc.) ou pas.\n",
        "\n",
        "Regardons ces types de violence plus en d√©tails :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_hour = df_hour.groupBy(['domestic', 'hour']).count().orderBy('hour').toPandas()"
      ],
      "metadata": {
        "id": "rtn14ImWNxC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for domestic cases\n",
        "domestic_cases = df_hour.filter(df_hour['domestic'] == True)\n",
        "\n",
        "# Group by hour and count\n",
        "domestic_cases_by_hour = domestic_cases.groupBy('hour').count().orderBy('hour').toPandas()\n",
        "\n",
        "# Create the bar plot\n",
        "#plt.figure(figsize=(8, 4))\n",
        "domestic_cases_by_hour.plot(kind='bar', x='hour', y='count', ax=plt.gca())\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Domestic Cases')\n",
        "plt.title('Number of Domestic Cases per Hour')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "loniimIfPLKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a une mont√©e des violences domestiques le soir, avec un pic isol√© √† midi (disputes pendant le repas ?)"
      ],
      "metadata": {
        "id": "5f7-S4h5PjGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et comment √ßa se situe par rapport aux crimes/violences \"non-domestiques\" ?"
      ],
      "metadata": {
        "id": "ozELYD2UPMKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Group data by hour and domestic status and sum the counts\n",
        "domestic_counts_grouped = domestic_hour.groupby(['hour', 'domestic'])['count'].sum().unstack()\n",
        "\n",
        "# Plot stacked bar chart\n",
        "domestic_counts_grouped.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.title('Hourly Crime Counts by Domestic Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Domestic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2qDAjdl7OSts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1PpuWiB11y"
      },
      "source": [
        "### Une analyse par rapport au temps\n",
        "\n",
        "Les donn√©es de type heure/date permettent d'obtenir plus d'information sur les types de crime et d'√©mettre des hypoth√®ses sur leurs sursauts. Par ontre, d'autres facteurs externes comme le changement de garde ou les nouvelles politiques de s√©curit√© peuvent avoir un impact non d√©crit ici.\n",
        "\n",
        "N√©anmoins, si on a une id√©e de quand et o√π les crimes sont les plus fr√©quents, on peut s'aventurer √† faire quelques pr√©visions..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8ZdrIGpB11y"
      },
      "source": [
        "On va rajouter quelques champs utiles :\n",
        "\n",
        " * l'heure du jour (d√©j√† pr√©sent dans le champ 'hour')\n",
        " * le jour de la semaine (dimanche = 1, ..., samedi = 7)\n",
        " * le mois de l'ann√©e\n",
        " * le \"num√©ro du jour\" dans une s√©quence 1, 2...(on commence √† compter √† partir du 2001-01-01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79-une_QB11y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import dayofweek, month, dayofmonth, datediff, to_date, lit\n",
        "\n",
        "df_dates = df_hour.withColumn('week_day', dayofweek(df_hour['date_time']))\\\n",
        "                 .withColumn('year_month', month(df_hour['date_time']))\\\n",
        "                 .withColumn('month_day', dayofmonth(df_hour['date_time']))\\\n",
        "                 .withColumn('date_number', datediff(df['date_time'], to_date(lit('2001-01-01'), format='yyyy-MM-dd')))\\\n",
        "                 .cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RKdwEk5B11y"
      },
      "outputs": [],
      "source": [
        "df_dates.select(['date', 'month', 'hour', 'week_day', 'year', 'year_month', 'month_day', 'date_number']).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmRBhvqaB11z"
      },
      "source": [
        "## Les crimes par rapport au jour de la semaine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl-mAsJWB11z"
      },
      "outputs": [],
      "source": [
        "week_day_crime_counts = df_dates.groupBy('week_day').count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "week_day_crime_counts_pddf = week_day_crime_counts.orderBy('week_day').toPandas()"
      ],
      "metadata": {
        "id": "eQWboGPbQToO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIqgh5QZB11z"
      },
      "outputs": [],
      "source": [
        "week_day_crime_counts_pddf.plot(kind='bar', x='week_day', y='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On voit tr√®s peu de variance... D'un autre c√¥t√©, les criminels restent \"m√©chants\" tous les jours. Et probablemnt il y a des crimes le dimanche qui ne sont signal√©s que le lundi !"
      ],
      "metadata": {
        "id": "bmSWboILXkFd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1POAD8TAB11z"
      },
      "source": [
        "## Mois de l'ann√©e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy7XMSlTB11z"
      },
      "outputs": [],
      "source": [
        "year_month_crime_counts = df_dates.groupBy('year_month').count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf = year_month_crime_counts.orderBy('year_month').toPandas()"
      ],
      "metadata": {
        "id": "yil8heVQRXk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdsyEb-B110"
      },
      "source": [
        "Il semble que la p√©riode Mai-Ao√ªt est la plus active pour les criminels. Des id√©es sur la cause ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_GjvI9lB110"
      },
      "outputs": [],
      "source": [
        "year_month_crime_counts_pddf.plot(y='count', x='year_month', kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AH, √ßa c'est int√©ressant ! On a beaucoup de crimes en janvier et f√©vrier. Serait-√ßa li√© √† la d√©prim de l'hiver ? Regardons rapidement si √ßa a un impact sur les violences domestiques."
      ],
      "metadata": {
        "id": "xt5zHkF5SH3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_month = df_dates.groupBy('domestic','year_month').count().orderBy('year_month').toPandas()"
      ],
      "metadata": {
        "id": "iS3NJuJ0StNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_month = domestic_month[domestic_month['domestic'] == True]"
      ],
      "metadata": {
        "id": "mhYUzcIHTFSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf['domestic_count']=domestic_month['count']\n",
        "year_month_crime_counts_pddf['domestic_percent'] = domestic_month['count']/year_month_crime_counts_pddf['count']"
      ],
      "metadata": {
        "id": "U26Iz7Okkfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf.plot(x='year_month', y='domestic_percent', kind='bar', color='orange')"
      ],
      "metadata": {
        "id": "ixpsd_6OT1rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien que les mois d'hiver pr√©sentent un taux √©lev√© de violences domestiques, le \"blues de l'hiver\" ne semble pas avoir une influence si grande que √ßa. üòØ"
      ],
      "metadata": {
        "id": "Ecx7H6BhlSkU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB_Qsm4qB118"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Pouvons-nous pr√©dire la cat√©gorie de crime (`primary_type`) √† partir de quelques caract√©ristiques (domestique, avec violence), l'endroit (district, community_area), etc. ?\n",
        "\n",
        "Afin de le faire, on va nettoyer un peu plus le dataset.\n",
        "\n",
        "Tout d'abord, essayons de supprimer quelques colonnes qui ne sont pas int√©ressantes ou qui risquent d'influencer trop le dataset :\n",
        "\n",
        " * 'id'\n",
        " * 'case_number'\n",
        " * 'date' - d√©j√† pr√©sent dans les autres donn√©es de date/heure\n",
        " * 'block' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'iucr' - c'est juste un code pour le type de crime\n",
        " * 'x_coordinate' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'y_coordinate' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'year' - d√©j√† pr√©sent dans les autres donn√©es de date/heure\n",
        " * 'updated_on' - pas utile\n",
        " * 'latitude' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'longitude' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'location' - non inclus\n",
        " * 'date_time' - trop pr√©cis, peut ajouter du \"bruit\"\n",
        " * 'description' - Supprim√©. On trouvera l'√©quivalent dans `primary type`, qui est **notre objectif**\n",
        "\n",
        " On pourrait faire √ßa avec des `drop()`, mais faisons diff√©remment : avec Spark nous avons la fonction `select()`, donc essayons de passer plut√¥t la liste de features qu'on veut garder :\n",
        "\n",
        " * 'location_description'\n",
        " * 'arrest'\n",
        " * 'domestic'\n",
        " * 'beat'\n",
        " * 'district'\n",
        " * 'ward'\n",
        " * 'community_area'\n",
        " * 'fbi_code'\n",
        " * 'hour'\n",
        " * 'week_day'\n",
        " * 'year_month'\n",
        " * 'month_day'\n",
        " * 'date_number'\n",
        " * 'primary_type'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7PAhZerB118"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        " 'location_description',\n",
        " 'arrest',\n",
        " 'domestic',\n",
        " 'beat',\n",
        " 'district',\n",
        " 'ward',\n",
        " 'community_area',\n",
        " 'fbi_code',\n",
        " 'hour',\n",
        " 'week_day',\n",
        " 'year_month',\n",
        " 'month_day',\n",
        " 'date_number',\n",
        " 'primary_type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoeTVTzFB118"
      },
      "outputs": [],
      "source": [
        "features_df = df_dates.select(selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons aussi identifier les types \"uniques\" pour les diff√©rents types de features (quels types de \"location_description\", quels types de \"arrest\"...). √áa sera utile pour la conversion des donn√©es cat√©goriques."
      ],
      "metadata": {
        "id": "Z4EL07LymN9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAaaC4YIB118"
      },
      "outputs": [],
      "source": [
        "feature_level_count_dic = []\n",
        "\n",
        "for feature in selected_features:\n",
        "    print('Analysing %s' % feature)\n",
        "    levels_list_df = features_df.select(feature).distinct()\n",
        "    feature_level_count_dic.append({'feature': feature, 'level_count': levels_list_df.count()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rSiIDjeB119"
      },
      "outputs": [],
      "source": [
        "pnd.DataFrame(feature_level_count_dic).sort_values(by='level_count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCp6KhLB119"
      },
      "source": [
        "### Preparer le mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On fait un premier passage pour supprimer les \"cases vides\" :"
      ],
      "metadata": {
        "id": "oAwyH7Uyq69B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features = features_df.dropna(subset=selected_features)"
      ],
      "metadata": {
        "id": "IF-tR7RZq-Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.show(5)"
      ],
      "metadata": {
        "id": "SvGMBYRonDpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.printSchema()"
      ],
      "metadata": {
        "id": "8noZltbOu8Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les features retenues sont des **cat√©gories**, donc nous devons passer par un encodeur pour les transformer en valeurs num√©riques.\n",
        "\n",
        "Sur ScikitLearn on pourrait utiliser `OrdinalEncoder`ou `OneHotEncoder`, mais √ßa risque de ne pas fonctionner si on a beaucoup de donn√©es.\n",
        "\n",
        "Spark offre ses propres versions d'encodeurs. Ici, nous voulons utiliser `StringIndexer`, un encodeur qui fonctionne comme OrdinalEncoder de sklearn."
      ],
      "metadata": {
        "id": "h_pmxPb3p6nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention : StringIndexer ne reconna√Æt que les colonnes de format String (üôÉ). Il faudra transformer les colonnes *arrest* et *domestic* en string, car pour le moment elles sont de type bool√©en !"
      ],
      "metadata": {
        "id": "YNt6y-oIuiUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features = df_dates_features.withColumn('arrest', df_dates_features['arrest'].cast('string'))\n",
        "df_dates_features = df_dates_features.withColumn('domestic', df_dates_features['domestic'].cast('string'))"
      ],
      "metadata": {
        "id": "HKTnduYpvIbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzcDGH9xB119"
      },
      "source": [
        "L√† √ßa doit √™tre bon !\n",
        "\n",
        "Utilisons le string indexer de Spark pour transformer les cat√©gories des features s√©l√©ctionn√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzIUmUKVB119"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv1HInedB119"
      },
      "outputs": [],
      "source": [
        "for feature in feature_level_count_dic:\n",
        "    indexer = StringIndexer(inputCol=feature['feature'], outputCol='%s_indexed' % feature['feature'])\n",
        "    print('Fitting feature \"%s\"' % feature['feature'])\n",
        "    model = indexer.fit(df_dates_features)\n",
        "    print('Transforming \"%s\"' % feature['feature'])\n",
        "    df_dates_features = model.transform(df_dates_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme on peut voir, on vient de cr√©er plusieurs colonnes supp√©lentaires (suffixe _indexed) qui contiennent des valeurs num√©riques."
      ],
      "metadata": {
        "id": "eFBEmO6kwSrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.show(5)"
      ],
      "metadata": {
        "id": "AX0EoVZxwA9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTTRMWzvB11-"
      },
      "source": [
        "\n",
        "Maintenant, on va vectoriser les √©l√©ments pour les avoir dans une colonne `features`. En effet, Spark n'utilise pas des Dataframe directement mais a besoin qu'on transforme les donn√©es en vecteurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQlyvrqDB11-"
      },
      "outputs": [],
      "source": [
        "indexed_features = ['%s_indexed' % fc['feature'] for fc in feature_level_count_dic]\n",
        "indexed_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd7cHSaDB11-"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=indexed_features, outputCol='features')\n",
        "vectorized_df_dates = assembler.transform(df_dates_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjBaOvIB11-"
      },
      "outputs": [],
      "source": [
        "vectorized_df_dates.select('features').take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VhRLNToB11_"
      },
      "source": [
        "### Et entra√Æner le mod√®le.\n",
        "\n",
        "Utiliser une r√©partition **60%**/**40%** entre les donn√©es train et test.\n",
        "\n",
        "Pour commencer, utilisons une r√©gression logistique.\n",
        "On peut voir l'ensemble de m√©thodes support√©es par [Spark ici.](https://spark.apache.org/docs/latest/ml-classification-regression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5o3nfGoB11_"
      },
      "outputs": [],
      "source": [
        "train, test = vectorized_df_dates.randomSplit([0.6, 0.4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiOrrAxB11_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN_6tIczB12D"
      },
      "outputs": [],
      "source": [
        "logisticRegression = LogisticRegression(labelCol='primary_type_indexed', featuresCol='features', maxIter=10, family='multinomial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1POpewVB12D"
      },
      "outputs": [],
      "source": [
        "fittedModel = logisticRegression.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Bp4ZAPB12D"
      },
      "source": [
        "## Quelle est la performance du mod√®le ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUk1afIxB12D"
      },
      "outputs": [],
      "source": [
        "fittedModel.summary.accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqVSfg5AB12I"
      },
      "source": [
        "#### Est-ce que √ßa semble un bon mod√®le pour pr√©dire les crimes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zia6icgeB12I"
      },
      "source": [
        "## √Ä vous:\n",
        "\n",
        " * Ex√©cuter le mod√®le sur l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = fittedModel.transform(test)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"primary_type_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy on test data = {accuracy}\")"
      ],
      "metadata": {
        "id": "Gv1zIVle17xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a environ 83% d'accuracy. Ceci dit, sur 30 cat√©gories de `primary_index`, il y a certaines qui doivent √™tre tr√®s bien pr√©dites et d'autres pas."
      ],
      "metadata": {
        "id": "rqlqR-fY2p9j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45O-77-s18Xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "chicago-crime-data-on-spark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}