{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/RT0902-IntroML/blob/main/15-Chicago_crime_data_on_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5NGFmN8B11c"
      },
      "source": [
        "# Chicago crime dataset analysis\n",
        "---\n",
        "\n",
        "Ce notebook permet d'appliquer un peu de vos connaissances à la découverte d'un vrai dataset.\n",
        "\n",
        "Vous allez effectuer :\n",
        " * Lecture, transformation et requêtage avec Apache Spark.\n",
        " * Parfois, transformer les données en Pandas pour une meilleure visualisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orsg3O-SB11e"
      },
      "source": [
        "---\n",
        "\n",
        "## Quelques Import\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zbNyaRuB11f"
      },
      "source": [
        "Import de Pandas et Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQft1Nw1B11g"
      },
      "outputs": [],
      "source": [
        "## standard imports\n",
        "import pandas as pnd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWzoN7XpB11e"
      },
      "source": [
        "Spark imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG1K0VaslXFL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "memory = '8g'\n",
        "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewSQBpTqB11f"
      },
      "outputs": [],
      "source": [
        "## spark imports\n",
        "from pyspark.sql import Row, SparkSession\n",
        "import pyspark.sql.functions as pyf\n",
        "\n",
        "#spark = SparkSession.builder.master(\"local[1]\").appName(\"RT0902\").getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"RT0902\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkJN_E5AB11h"
      },
      "source": [
        "---\n",
        "## Dataset\n",
        "Les données originales viennent de Kaggle (https://www.kaggle.com/djonafegnem/chicago-crime-data-analysis)\n",
        "\n",
        "On trouve une liste de crimes registrés par le département de police de Chicago.\n",
        "\n",
        "Le dataset \"réel\" contient 4 fichiers pour des crimes allant de 2001 à 2017.\n",
        "Comme le traitement de ces fichiers est long et demandeur en ressources, vous allez d'abord travailler avec un fichier réduit, qui ne contient que des données de 2001.\n",
        "\n",
        "Une fois votre code \"validé\", vous pouvez l'utiliser sur le cloud pour traiter l'ensemble de fichiers de la police.\n",
        "\n",
        "Ci-dessous vous trovez une description des différents champs des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2F-rS7vB11h"
      },
      "outputs": [],
      "source": [
        "content_cols = '''\n",
        "ID - Unique identifier for the record.\n",
        "Case Number - The Chicago Police Department RD Number (Records Division Number), which is unique to the incident.\n",
        "Date - Date when the incident occurred. this is sometimes a best estimate.\n",
        "Block - The partially redacted address where the incident occurred, placing it on the same block as the actual address.\n",
        "IUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.\n",
        "Primary Type - The primary description of the IUCR code.\n",
        "Description - The secondary description of the IUCR code, a subcategory of the primary description.\n",
        "Location Description - Description of the location where the incident occurred.\n",
        "Arrest - Indicates whether an arrest was made.\n",
        "Domestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\n",
        "Beat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area – each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.\n",
        "District - Indicates the police district where the incident occurred. See the districts at https://data.cityofchicago.org/d/fthy-xz3r.\n",
        "Ward - The ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.\n",
        "Community Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https://data.cityofchicago.org/d/cauq-8yn6.\n",
        "FBI Code - Indicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\n",
        "X Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Y Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Year - Year the incident occurred.\n",
        "Updated On - Date and time the record was last updated.\n",
        "Latitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Longitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
        "Location - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEV6TMBnB11k"
      },
      "source": [
        "### Données\n",
        "\n",
        "Les données seront téléchargées et stockées dans `./data/`. Ce sont des fichiers .CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dzWdo_JWJgw"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q9V0tdZWvN5"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r gs://angelo_crime_data/*.csv ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyafw6P0B11l"
      },
      "outputs": [],
      "source": [
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYYYo3wB11l"
      },
      "source": [
        "---\n",
        "## Lecture des données\n",
        "\n",
        "Avec l'opération `csv read` de spark, nous allons lire et parser les fichiers. Le résultat sera un seul DataFrame :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM8fs9TgB11l"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.csv('gs://angelo_crime_data/Chicago_*.csv', inferSchema=True, header=True)\n",
        "df = spark.read.csv('data/mini_data.csv', inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz6WzWjsl1H1"
      },
      "source": [
        "Note : ce qui prend vraiment le temps est la découverte du schéma : on n'a pas tellement de lignes, après tout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lSK8YawB11m"
      },
      "outputs": [],
      "source": [
        "# Affichage du schéma (structure) du dataframe\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Différences entre Pandas et Spark\n",
        "Pandas a des opérations telles que `info()` et `describe()`. Dans Spark, on n'a que `describe()`, qui n'est pas comparable à celle de Pandas : il affiche plutôt le type des données, un peu comme `printSchema()`."
      ],
      "metadata": {
        "id": "ri4XZLFrDMu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "_PjKFVEaDkY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi9n1CYaB11n"
      },
      "source": [
        "Certaines lignes de n'ont aucune valeur déclarée à la colonne `location_description` . C'est le moment de les supprimer.\n",
        "\n",
        "Pour cela, nous allons filtrer les entrées vides ('') en utilisant la fonction **`Dataset.filter`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LQyYPtcbtnQ"
      },
      "outputs": [],
      "source": [
        "df = df.filter(df['location_description'] != '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_YRLdkB11n"
      },
      "source": [
        "Un petit aperçu du début du dataframe :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym9AaXn0B11n"
      },
      "outputs": [],
      "source": [
        "df.show(n=3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a quand même plus de 560 mille entrées sur le petit fichier !! 😵"
      ],
      "metadata": {
        "id": "XhAVXvqjEuvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Ek_ORlB11o"
      },
      "outputs": [],
      "source": [
        "print(df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykY8IxXaB11o"
      },
      "source": [
        "---\n",
        "## Comprendre les données"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types de Crime\n",
        "\n",
        "On veut connaître combien de types de crime (catégories) existent dans le fichier."
      ],
      "metadata": {
        "id": "Mue99rjLC2-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_8EqKGnB11o"
      },
      "outputs": [],
      "source": [
        "# crime types\n",
        "crime_type_groups = df.groupBy('primary_type').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB7L2nGJB11o"
      },
      "outputs": [],
      "source": [
        "crime_type_counts = crime_type_groups.orderBy('count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrBK30fB11o"
      },
      "source": [
        "Jusqu'à ici ça a été rapide : Spark fait une exécution *lazy*, i.e., il n'a fait qu'enregistrer les *transformations* à applier. Il attendra pour lancer l'exécution uniquement lorsqu'une *action* est demandée (par exemple, afficher le résultat).\n",
        "\n",
        "Dans la ligne suivante on demande le nombre total de lignes, mais en fait il va appliquer les modifications, faire le filtrage, etc. Sur un grand dataset, ça peut prendre pas mal de temps (d'où l'intérêt de distribuer le travail entre plusieurs machines)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5allZo4IB11p"
      },
      "source": [
        "\n",
        "\n",
        "La commande suivante affiche les 20 types de crime les plus fréquents :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0G89nlsB11q"
      },
      "outputs": [],
      "source": [
        "crime_type_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nX9DLuuB11q"
      },
      "source": [
        "On peut faire un affichage plus propre (et d'autres opérations) en transformant ce dataframe en Pandas :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om_CpQ58oV0v"
      },
      "outputs": [],
      "source": [
        "counts_pddf = crime_type_counts.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMYfXn6obj0"
      },
      "outputs": [],
      "source": [
        "counts_pddf.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce dataset Pandas peut être utilisé pour une petite visualisation :"
      ],
      "metadata": {
        "id": "wX93myvEF4X1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeJrB5AIB11r",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
        "\n",
        "counts_pddf.sort_values('count').plot(kind='barh', x='primary_type', y='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya0LZ2NpB11r"
      },
      "source": [
        "### Convertir les dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbEroH7VB11s"
      },
      "source": [
        "**Si vous avez été attentif**, vous avez remarqué que les colonnes avec des dates ont été lues comme du texte simple :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "kdlf1-KDGh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2QC39keB11s"
      },
      "source": [
        "---\n",
        "En effet, le schéma montrait que le champ `date` était de type `string`, ce qui n'est pas très utile.\n",
        "\n",
        "Nous allons convertir ces dates au format timestamp.\n",
        "\n",
        "Nous allons changer ce format afin de le lire sous la forme '02/23/2006 09:06:22 PM' , c'est à dire **`'MM/dd/yyyy hh:mm:ss a'`** (format américain).\n",
        "\n",
        "On va aussi rajouter une colonne `month` qui indique le premier jour du mois, sans l'heure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYzZl6iiB11s"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_timestamp, hour, trunc\n",
        "# d'abord, on convertit 'date' avec to_timestamp() et on enregistre cette valeur dans 'date_time'\n",
        "df = df.withColumn('date_time', to_timestamp('date', 'MM/dd/yyyy hh:mm:ss a'))\n",
        "# ensuite, on crée une colonne 'month' à partir de 'datetime')\n",
        "df = df.withColumn('month', trunc('date_time', 'month')) #adding a month column to be able to view stats on a monthly basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u35zw7vgB11s"
      },
      "outputs": [],
      "source": [
        "df.select(['date','date_time', 'month'])\\\n",
        "  .show(n=20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoeD7ctRB11s"
      },
      "source": [
        "### Combien d'arrestations ?\n",
        "\n",
        "Pas tous les crimes donnent lieu à des arrestations.\n",
        "Grâce à `groupBy`, nous allons afficher le nombre d'arrestations par mois (et le nombre de crimes sans arrestations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a-2HrFHB11s"
      },
      "outputs": [],
      "source": [
        "# On peut utiliser la colonne month pour affiche les quantités d'arrestations au fil des années, groupées par mois :\n",
        "type_arrest_date = df.groupBy(['arrest', 'month'])\\\n",
        "                     .count()\\\n",
        "                     .orderBy(['month', 'count'], ascending=[True, False])\n",
        "print()\n",
        "type_arrest_date.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox4oZ41EB11u"
      },
      "source": [
        "### Comment le nombre d'arrestations a évolué sur l'année ?\n",
        "\n",
        "Pour l'afichage, nous allons encore une fois transformer notre dataset en Pandas. On transforme `type_arrest_date`, puis on utilise matplotlib pour l'affichage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: à partir de type_arrest_date, générer un graphique matplotlib affichant le nombre arrests true et false, par mois\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for easier plotting\n",
        "type_arrest_date_pandas = type_arrest_date.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for arrest_status in type_arrest_date_pandas['arrest'].unique():\n",
        "    subset = type_arrest_date_pandas[type_arrest_date_pandas['arrest'] == arrest_status]\n",
        "    plt.plot(subset['month'], subset['count'], label=f'Arrest: {arrest_status}')\n",
        "\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Arrests')\n",
        "plt.title('Number of Arrests (True/False) per Month')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "08Ja7Z_YI6aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVUmuvVIB11u"
      },
      "source": [
        "### À quel moment de la journée les criminels sont plus actifs ?\n",
        "\n",
        "Ici c'est à vous de refaire le même type d'opération. Je vais juste vous montrer comment créer une colonne avec les heures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHDjY5bBB11v"
      },
      "outputs": [],
      "source": [
        "# Extract the \"hour\" field from the date into a separate column called \"hour\"\n",
        "df_hour = df.withColumn('hour', hour(df['date_time']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsZNEJXZB11v"
      },
      "outputs": [],
      "source": [
        "# Derive a data frame with crime counts per hour of the day:\n",
        "hourly_count = df_hour.groupBy(['primary_type', 'hour']).count()\n",
        "hourly_total_count = hourly_count.groupBy('hour').sum('count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVeHieTYB11v"
      },
      "outputs": [],
      "source": [
        "hourly_count_pddf = hourly_count.toPandas()\n",
        "hourly_total_count_pddf = hourly_total_count.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGf_KjlJB11v"
      },
      "outputs": [],
      "source": [
        "hourly_count_pddf = hourly_count_pddf.sort_values(by='hour')\n",
        "hourly_total_count_pddf = hourly_total_count_pddf.sort_values(by='hour')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_count_pddf.head(10)"
      ],
      "metadata": {
        "id": "mu728Fx4LU3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_total_count_pddf.head(10)"
      ],
      "metadata": {
        "id": "A4lzRYWNKKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S08QHOlBB11v"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(hourly_total_count_pddf['hour'], hourly_total_count_pddf['sum(count)'], label='Hourly Count')\n",
        "\n",
        "ax.set(xlabel='Hour of Day', ylabel='Total records',\n",
        "       title='Overall hourly crime numbers')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMZ2MN6dB11v"
      },
      "source": [
        "Il semble que c'est plus agité entre 18h et 22h... avec un pic à midi.\n",
        "\n",
        "Regardons le détail de chaque type de crime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group data by hour and primary type and sum the counts\n",
        "hourly_counts_grouped = hourly_count_pddf.groupby(['hour', 'primary_type'])['count'].sum().unstack()\n",
        "\n",
        "# Plot stacked area chart\n",
        "hourly_counts_grouped.plot(kind='area', stacked=True, ax=plt.gca())\n",
        "\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.title('Hourly Crime Counts by Primary Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Primary Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tGpzIUylLfCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6rHrROiB11w"
      },
      "source": [
        "### Dans que type d'endroit les crimes sont commis ?\n",
        "\n",
        "Le dataset indique la \"classe\" de lieu où le crime a été commis : maison, rue, etc. Regardons ça en détails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBdTg6GRB11w"
      },
      "outputs": [],
      "source": [
        "# Combien de types d'endroit sont recensés\n",
        "df.select('location_description').distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsllA6BgB11w"
      },
      "source": [
        "Ok, il y a 114 types différents d'endroit qui sont recensés.\n",
        "\n",
        "Quels sont les 10 endroits les plus fréquents ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OpMDWEBB11w"
      },
      "outputs": [],
      "source": [
        "df.groupBy(['location_description']).count().orderBy('count', ascending=False).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey4LQDQWB11x"
      },
      "source": [
        "### Crimes \"domestiques\" :\n",
        "\n",
        "En dehors de la localité, le dataset indique aussi s'il s'agit d'une violence domestique (dispute familiale, harcélement, etc.) ou pas.\n",
        "\n",
        "Regardons ces types de violence plus en détails :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_hour = df_hour.groupBy(['domestic', 'hour']).count().orderBy('hour').toPandas()"
      ],
      "metadata": {
        "id": "rtn14ImWNxC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for domestic cases\n",
        "domestic_cases = df_hour.filter(df_hour['domestic'] == True)\n",
        "\n",
        "# Group by hour and count\n",
        "domestic_cases_by_hour = domestic_cases.groupBy('hour').count().orderBy('hour').toPandas()\n",
        "\n",
        "# Create the bar plot\n",
        "#plt.figure(figsize=(8, 4))\n",
        "domestic_cases_by_hour.plot(kind='bar', x='hour', y='count', ax=plt.gca())\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Domestic Cases')\n",
        "plt.title('Number of Domestic Cases per Hour')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "loniimIfPLKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a une montée des violences domestiques le soir, avec un pic isolé à midi (disputes pendant le repas ?)"
      ],
      "metadata": {
        "id": "5f7-S4h5PjGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et comment ça se situe par rapport aux crimes/violences \"non-domestiques\" ?"
      ],
      "metadata": {
        "id": "ozELYD2UPMKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Group data by hour and domestic status and sum the counts\n",
        "domestic_counts_grouped = domestic_hour.groupby(['hour', 'domestic'])['count'].sum().unstack()\n",
        "\n",
        "# Plot stacked bar chart\n",
        "domestic_counts_grouped.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.title('Hourly Crime Counts by Domestic Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Domestic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2qDAjdl7OSts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1PpuWiB11y"
      },
      "source": [
        "### Une analyse par rapport au temps\n",
        "\n",
        "Les données de type heure/date permettent d'obtenir plus d'information sur les types de crime et d'émettre des hypothèses sur leurs sursauts. Par ontre, d'autres facteurs externes comme le changement de garde ou les nouvelles politiques de sécurité peuvent avoir un impact non décrit ici.\n",
        "\n",
        "Néanmoins, si on a une idée de quand et où les crimes sont les plus fréquents, on peut s'aventurer à faire quelques prévisions..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8ZdrIGpB11y"
      },
      "source": [
        "On va rajouter quelques champs utiles :\n",
        "\n",
        " * l'heure du jour (déjà présent dans le champ 'hour')\n",
        " * le jour de la semaine (dimanche = 1, ..., samedi = 7)\n",
        " * le mois de l'année\n",
        " * le \"numéro du jour\" dans une séquence 1, 2...(on commence à compter à partir du 2001-01-01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79-une_QB11y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import dayofweek, month, dayofmonth, datediff, to_date, lit\n",
        "\n",
        "df_dates = df_hour.withColumn('week_day', dayofweek(df_hour['date_time']))\\\n",
        "                 .withColumn('year_month', month(df_hour['date_time']))\\\n",
        "                 .withColumn('month_day', dayofmonth(df_hour['date_time']))\\\n",
        "                 .withColumn('date_number', datediff(df['date_time'], to_date(lit('2001-01-01'), format='yyyy-MM-dd')))\\\n",
        "                 .cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RKdwEk5B11y"
      },
      "outputs": [],
      "source": [
        "df_dates.select(['date', 'month', 'hour', 'week_day', 'year', 'year_month', 'month_day', 'date_number']).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmRBhvqaB11z"
      },
      "source": [
        "## Les crimes par rapport au jour de la semaine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl-mAsJWB11z"
      },
      "outputs": [],
      "source": [
        "week_day_crime_counts = df_dates.groupBy('week_day').count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "week_day_crime_counts_pddf = week_day_crime_counts.orderBy('week_day').toPandas()"
      ],
      "metadata": {
        "id": "eQWboGPbQToO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIqgh5QZB11z"
      },
      "outputs": [],
      "source": [
        "week_day_crime_counts_pddf.plot(kind='bar', x='week_day', y='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On voit très peu de variance... D'un autre côté, les criminels restent \"méchants\" tous les jours. Et probablemnt il y a des crimes le dimanche qui ne sont signalés que le lundi !"
      ],
      "metadata": {
        "id": "bmSWboILXkFd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1POAD8TAB11z"
      },
      "source": [
        "## Mois de l'année\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy7XMSlTB11z"
      },
      "outputs": [],
      "source": [
        "year_month_crime_counts = df_dates.groupBy('year_month').count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf = year_month_crime_counts.orderBy('year_month').toPandas()"
      ],
      "metadata": {
        "id": "yil8heVQRXk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdsyEb-B110"
      },
      "source": [
        "Il semble que la période Mai-Août est la plus active pour les criminels. Des idées sur la cause ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_GjvI9lB110"
      },
      "outputs": [],
      "source": [
        "year_month_crime_counts_pddf.plot(y='count', x='year_month', kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AH, ça c'est intéressant ! On a beaucoup de crimes en janvier et février. Serait-ça lié à la déprim de l'hiver ? Regardons rapidement si ça a un impact sur les violences domestiques."
      ],
      "metadata": {
        "id": "xt5zHkF5SH3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_month = df_dates.groupBy('domestic','year_month').count().orderBy('year_month').toPandas()"
      ],
      "metadata": {
        "id": "iS3NJuJ0StNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domestic_month = domestic_month[domestic_month['domestic'] == True]"
      ],
      "metadata": {
        "id": "mhYUzcIHTFSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf['domestic_count']=domestic_month['count']\n",
        "year_month_crime_counts_pddf['domestic_percent'] = domestic_month['count']/year_month_crime_counts_pddf['count']"
      ],
      "metadata": {
        "id": "U26Iz7Okkfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_crime_counts_pddf.plot(x='year_month', y='domestic_percent', kind='bar', color='orange')"
      ],
      "metadata": {
        "id": "ixpsd_6OT1rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien que les mois d'hiver présentent un taux élevé de violences domestiques, le \"blues de l'hiver\" ne semble pas avoir une influence si grande que ça. 😯"
      ],
      "metadata": {
        "id": "Ecx7H6BhlSkU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB_Qsm4qB118"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Pouvons-nous prédire la catégorie de crime (`primary_type`) à partir de quelques caractéristiques (domestique, avec violence), l'endroit (district, community_area), etc. ?\n",
        "\n",
        "Afin de le faire, on va nettoyer un peu plus le dataset.\n",
        "\n",
        "Tout d'abord, essayons de supprimer quelques colonnes qui ne sont pas intéressantes ou qui risquent d'influencer trop le dataset :\n",
        "\n",
        " * 'id'\n",
        " * 'case_number'\n",
        " * 'date' - déjà présent dans les autres données de date/heure\n",
        " * 'block' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'iucr' - c'est juste un code pour le type de crime\n",
        " * 'x_coordinate' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'y_coordinate' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'year' - déjà présent dans les autres données de date/heure\n",
        " * 'updated_on' - pas utile\n",
        " * 'latitude' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'longitude' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'location' - non inclus\n",
        " * 'date_time' - trop précis, peut ajouter du \"bruit\"\n",
        " * 'description' - Supprimé. On trouvera l'équivalent dans `primary type`, qui est **notre objectif**\n",
        "\n",
        " On pourrait faire ça avec des `drop()`, mais faisons différemment : avec Spark nous avons la fonction `select()`, donc essayons de passer plutôt la liste de features qu'on veut garder :\n",
        "\n",
        " * 'location_description'\n",
        " * 'arrest'\n",
        " * 'domestic'\n",
        " * 'beat'\n",
        " * 'district'\n",
        " * 'ward'\n",
        " * 'community_area'\n",
        " * 'fbi_code'\n",
        " * 'hour'\n",
        " * 'week_day'\n",
        " * 'year_month'\n",
        " * 'month_day'\n",
        " * 'date_number'\n",
        " * 'primary_type'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7PAhZerB118"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        " 'location_description',\n",
        " 'arrest',\n",
        " 'domestic',\n",
        " 'beat',\n",
        " 'district',\n",
        " 'ward',\n",
        " 'community_area',\n",
        " 'fbi_code',\n",
        " 'hour',\n",
        " 'week_day',\n",
        " 'year_month',\n",
        " 'month_day',\n",
        " 'date_number',\n",
        " 'primary_type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoeTVTzFB118"
      },
      "outputs": [],
      "source": [
        "features_df = df_dates.select(selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons aussi identifier les types \"uniques\" pour les différents types de features (quels types de \"location_description\", quels types de \"arrest\"...). Ça sera utile pour la conversion des données catégoriques."
      ],
      "metadata": {
        "id": "Z4EL07LymN9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAaaC4YIB118"
      },
      "outputs": [],
      "source": [
        "feature_level_count_dic = []\n",
        "\n",
        "for feature in selected_features:\n",
        "    print('Analysing %s' % feature)\n",
        "    levels_list_df = features_df.select(feature).distinct()\n",
        "    feature_level_count_dic.append({'feature': feature, 'level_count': levels_list_df.count()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rSiIDjeB119"
      },
      "outputs": [],
      "source": [
        "pnd.DataFrame(feature_level_count_dic).sort_values(by='level_count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCp6KhLB119"
      },
      "source": [
        "### Preparer le modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On fait un premier passage pour supprimer les \"cases vides\" :"
      ],
      "metadata": {
        "id": "oAwyH7Uyq69B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features = features_df.dropna(subset=selected_features)"
      ],
      "metadata": {
        "id": "IF-tR7RZq-Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.show(5)"
      ],
      "metadata": {
        "id": "SvGMBYRonDpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.printSchema()"
      ],
      "metadata": {
        "id": "8noZltbOu8Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les features retenues sont des **catégories**, donc nous devons passer par un encodeur pour les transformer en valeurs numériques.\n",
        "\n",
        "Sur ScikitLearn on pourrait utiliser `OrdinalEncoder`ou `OneHotEncoder`, mais ça risque de ne pas fonctionner si on a beaucoup de données.\n",
        "\n",
        "Spark offre ses propres versions d'encodeurs. Ici, nous voulons utiliser `StringIndexer`, un encodeur qui fonctionne comme OrdinalEncoder de sklearn."
      ],
      "metadata": {
        "id": "h_pmxPb3p6nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention : StringIndexer ne reconnaît que les colonnes de format String (🙃). Il faudra transformer les colonnes *arrest* et *domestic* en string, car pour le moment elles sont de type booléen !"
      ],
      "metadata": {
        "id": "YNt6y-oIuiUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features = df_dates_features.withColumn('arrest', df_dates_features['arrest'].cast('string'))\n",
        "df_dates_features = df_dates_features.withColumn('domestic', df_dates_features['domestic'].cast('string'))"
      ],
      "metadata": {
        "id": "HKTnduYpvIbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzcDGH9xB119"
      },
      "source": [
        "Là ça doit être bon !\n",
        "\n",
        "Utilisons le string indexer de Spark pour transformer les catégories des features séléctionnées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzIUmUKVB119"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv1HInedB119"
      },
      "outputs": [],
      "source": [
        "for feature in feature_level_count_dic:\n",
        "    indexer = StringIndexer(inputCol=feature['feature'], outputCol='%s_indexed' % feature['feature'])\n",
        "    print('Fitting feature \"%s\"' % feature['feature'])\n",
        "    model = indexer.fit(df_dates_features)\n",
        "    print('Transforming \"%s\"' % feature['feature'])\n",
        "    df_dates_features = model.transform(df_dates_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme on peut voir, on vient de créer plusieurs colonnes suppélentaires (suffixe _indexed) qui contiennent des valeurs numériques."
      ],
      "metadata": {
        "id": "eFBEmO6kwSrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates_features.show(5)"
      ],
      "metadata": {
        "id": "AX0EoVZxwA9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTTRMWzvB11-"
      },
      "source": [
        "\n",
        "Maintenant, on va vectoriser les éléments pour les avoir dans une colonne `features`. En effet, Spark n'utilise pas des Dataframe directement mais a besoin qu'on transforme les données en vecteurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQlyvrqDB11-"
      },
      "outputs": [],
      "source": [
        "indexed_features = ['%s_indexed' % fc['feature'] for fc in feature_level_count_dic]\n",
        "indexed_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd7cHSaDB11-"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=indexed_features, outputCol='features')\n",
        "vectorized_df_dates = assembler.transform(df_dates_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjBaOvIB11-"
      },
      "outputs": [],
      "source": [
        "vectorized_df_dates.select('features').take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VhRLNToB11_"
      },
      "source": [
        "### Et entraîner le modèle.\n",
        "\n",
        "Utiliser une répartition **60%**/**40%** entre les données train et test.\n",
        "\n",
        "Pour commencer, utilisons une régression logistique.\n",
        "On peut voir l'ensemble de méthodes supportées par [Spark ici.](https://spark.apache.org/docs/latest/ml-classification-regression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5o3nfGoB11_"
      },
      "outputs": [],
      "source": [
        "train, test = vectorized_df_dates.randomSplit([0.6, 0.4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiOrrAxB11_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN_6tIczB12D"
      },
      "outputs": [],
      "source": [
        "logisticRegression = LogisticRegression(labelCol='primary_type_indexed', featuresCol='features', maxIter=10, family='multinomial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1POpewVB12D"
      },
      "outputs": [],
      "source": [
        "fittedModel = logisticRegression.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Bp4ZAPB12D"
      },
      "source": [
        "## Quelle est la performance du modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUk1afIxB12D"
      },
      "outputs": [],
      "source": [
        "fittedModel.summary.accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqVSfg5AB12I"
      },
      "source": [
        "#### Est-ce que ça semble un bon modèle pour prédire les crimes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zia6icgeB12I"
      },
      "source": [
        "## À vous:\n",
        "\n",
        " * Exécuter le modèle sur l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = fittedModel.transform(test)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"primary_type_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy on test data = {accuracy}\")"
      ],
      "metadata": {
        "id": "Gv1zIVle17xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a environ 83% d'accuracy. Ceci dit, sur 30 catégories de `primary_index`, il y a certaines qui doivent être très bien prédites et d'autres pas."
      ],
      "metadata": {
        "id": "rqlqR-fY2p9j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45O-77-s18Xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "chicago-crime-data-on-spark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}